{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich KG using MutiAgent in parrallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiofiles\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow\n",
    "from llama_index.core.agent.workflow import AgentInput, AgentOutput, ToolCall, ToolCallResult, AgentStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"]=\"AIzaSyBuGAPWnqtxGoCBnSgF_jm8X74-0CSavsk\"\n",
    "llm = GoogleGenAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----- TOOL FUNCTIONS -----\n",
    "\n",
    "async def extract_file_content(file_relative_path: str, repo_base_path: str) -> str:\n",
    "    \"\"\"Extracts and returns the content of a file given its relative path from the repository base asynchronously.\"\"\"\n",
    "    absolute_path = os.path.join(repo_base_path, file_relative_path)\n",
    "    try:\n",
    "        async with aiofiles.open(absolute_path, mode='r', encoding='utf-8') as f:\n",
    "            content = await f.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            async with aiofiles.open(absolute_path, mode='r', encoding='latin-1') as f:\n",
    "                content = await f.read()\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file (tried multiple encodings): {str(e)}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"File not found: {absolute_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "def get_project_tree_string(root_path: str, prefix: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Recursively generates a tree-like string for the given directory.\n",
    "    Example output:\n",
    "        ├── folder1\n",
    "        │   ├── file1.py\n",
    "        │   └── file2.py\n",
    "        └── folder2\n",
    "            └── file3.py\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    try:\n",
    "        entries = os.listdir(root_path)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading directory {root_path}: {e}\"\n",
    "    \n",
    "    entries.sort()\n",
    "    entries_count = len(entries)\n",
    "    for index, entry in enumerate(entries):\n",
    "        full_path = os.path.join(root_path, entry)\n",
    "        is_last = (index == entries_count - 1)\n",
    "        connector = \"└── \" if is_last else \"├── \"\n",
    "        lines.append(prefix + connector + entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            extension_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            subtree = get_project_tree_string(full_path, extension_prefix)\n",
    "            if subtree:\n",
    "                lines.append(subtree)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def get_combined_file_content_with_tree(file_relative_path: str, repo_base_path: str) -> str:\n",
    "    \"\"\"Combines the project tree (as textual context) with the content of a file.\"\"\"\n",
    "    file_content = await extract_file_content(file_relative_path, repo_base_path)    \n",
    "    project_tree = get_project_tree_string(repo_base_path)\n",
    "    combined_content = (\n",
    "        \"Project Tree:\\n\"\n",
    "        \"-------------\\n\"\n",
    "        f\"{project_tree}\\n\\n\"\n",
    "        \"File Content:\\n\"\n",
    "        \"-------------\\n\"\n",
    "        f\"{file_content}\"\n",
    "    )\n",
    "    return combined_content\n",
    "\n",
    "\n",
    "async def generate_file_description(ctx: Context, description:str) ->str :\n",
    "    \"\"\"Usefull to generate detailed description of a file based on its code\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"file_description\"] = description\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Description recorded.\"\n",
    "\n",
    "async def generate_code_summary(ctx: Context, summary: str, need_analysis: bool) -> str:\n",
    "    \"\"\"Generates a detailed summary of a file's code and determines if further analysis is necessary based on the file's content.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"code_summary\" not in current_state:\n",
    "        current_state[\"code_summary\"] = {}\n",
    "    current_state[\"code_summary\"][\"code_summary\"] = summary\n",
    "    current_state[\"code_summary\"][\"need_analysis\"] =  need_analysis\n",
    "    await ctx.set(\"state\", current_state)    \n",
    "    return \"Summary recorded.\"\n",
    "\n",
    "\n",
    "async def analyze_complexity(ctx: Context, complexity_analysis: str) -> str:\n",
    "    \"\"\"Usefull to generate detailed analyze complexity of a file based on its code\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"complexity_analysis\"] = complexity_analysis\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"complexity analysis detailed.\"\n",
    "\n",
    "async def analyze_dependency(\n",
    "    ctx: Context,\n",
    "    source: str,\n",
    "    target: str,\n",
    "    full_path:str,\n",
    "    type: str,\n",
    "    external: bool,\n",
    "    description: str\n",
    ") -> str:\n",
    "    \"\"\"Analyzes code dependencies and records detailed dependency information as a structured list\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"dependency_analysis\" not in current_state:\n",
    "        current_state[\"dependency_analysis\"] = []\n",
    "    \n",
    "    dependency_item = {\n",
    "        \"source\": source,\n",
    "        \"target\": target,\n",
    "        \"full_path\":full_path,\n",
    "        \"type\": type,\n",
    "        \"external\": external,\n",
    "        \"description\":description\n",
    "    }\n",
    "    \n",
    "    current_state[\"dependency_analysis\"].append(dependency_item)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Dependency analysis completed with structured data.\"\n",
    "\n",
    "async def extract_class_block(\n",
    "        ctx: Context,\n",
    "        docstring: str,\n",
    "        class_name:str,\n",
    "        code: str,\n",
    "        description: str\n",
    "    ):\n",
    "    \"\"\"Usefull to Extracts and registers a class block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"classes\" not in current_state:\n",
    "        current_state[\"classes\"] = []\n",
    "    class_block = {\n",
    "        \"description\": description,\n",
    "        \"docstring\": docstring,\n",
    "        \"class\":class_name,\n",
    "        \"code\": code\n",
    "    }\n",
    "    current_state[\"classes\"].append(class_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Class block extraction completed\"\n",
    "\n",
    "\n",
    "async def extract_method_block(\n",
    "    ctx: Context,\n",
    "    docstring: str,\n",
    "    method_name: str,\n",
    "    code: str,\n",
    "    description: str\n",
    "):\n",
    "    \"\"\"Usefull to Extracts and registers a method block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"methods\" not in current_state:\n",
    "        current_state[\"methods\"] = []\n",
    "    method_block = {\n",
    "        \"description\": description,\n",
    "        \"docstring\": docstring,\n",
    "        \"method\": method_name,\n",
    "        \"code\": code\n",
    "    }\n",
    "    current_state[\"methods\"].append(method_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Method block extraction completed\"\n",
    "\n",
    "\n",
    "async def extract_script_block(\n",
    "    ctx: Context,\n",
    "    code: str,\n",
    "    description: str,\n",
    "    script_name:str,\n",
    "):\n",
    "    \"\"\"Usefull to Extracts and registers a script block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"scripts\" not in current_state:\n",
    "        current_state[\"scripts\"] = []\n",
    "    script_block = {\n",
    "        \"script_name\":script_name,\n",
    "        \"description\": description,\n",
    "        \"code\": code,\n",
    "    }\n",
    "    current_state[\"scripts\"].append(script_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Script block extraction completed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DESCRIPTION_PROMPT = \"\"\"\n",
    "Please generate a clear, concise, and professional description of the file’s purpose and functionality, based exclusively on the provided code.\n",
    "Your description should highlight the core functionality, key components, and any important details that define the code's role in the system.\n",
    "\"\"\"\n",
    "\n",
    "CODE_SUMMARY_PROMPT = \"\"\"\n",
    "Provide a high-level overview of what the code does, outlining its key functionality, main components, \n",
    "and purpose within the larger system. The summary should capture the essence of the code without delving into \n",
    "implementation details.\n",
    "\n",
    "After generating the summary, return a dictionary containing two keys:\n",
    "- 'summary': A string that provides a brief description of the code's functionality.\n",
    "- 'need_analysis': A boolean indicating whether this file requires further analysis. \n",
    "  - Return `True` if the file should proceed to the next agent for detailed analysis.\n",
    "  - Return `False` if the file should be skipped and no further analysis is needed (e.g., for Dockerfiles, README.md, .sh files).\n",
    "Return ONLY the JSON object with no additional text, markdown formatting, or code blocks.\n",
    "Example response: {\"summary\": \"This is a utility module for data processing\", \"need_analysis\": true}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "CODE_COMPLIXTY_PROMPT = \"\"\"\n",
    "Analyze the code complexity and provide a comprehensive assessment, focusing on factors such as:\n",
    "- Algorithmic complexity (e.g., time and space complexity).\n",
    "- Code structure and readability.\n",
    "- Potential bottlenecks or areas for optimization.\n",
    "- Overall maintainability and scalability.\n",
    "\n",
    "Provide insights into how the complexity of the code might impact its performance or future development.\n",
    "\"\"\"\n",
    "\n",
    "CODE_DEPENDENCY_PROMPT=\"\"\"\n",
    "Your task is to analyze the provided code and extract dependencies into a simple format that is easy to load into a Neo4j database.\n",
    "\n",
    "ANALYSIS PROCEDURE:\n",
    "1. Identify all imports (standard libraries, third-party packages, local modules).\n",
    "2. Detect function or class dependencies within the file.\n",
    "3. Find references to other project files and external libraries.\n",
    "4. For local dependencies, resolve import paths using the provided project tree, converting dotted paths into full relative paths.\n",
    "5. For each dependency, create a structured output that includes:\n",
    "    - 'source': The file where the dependency originates.\n",
    "    - 'target': The file/module being referenced.\n",
    "    - 'type': The type of dependency (e.g., 'import', 'usage', etc.).\n",
    "    - 'path': The full relative path to the dependency (e.g., `app/db/wait_for_db.py`).\n",
    "    - 'external': Whether the dependency is external (e.g., third-party packages like `requests`) or internal (use `true/false`).\n",
    "    - 'description': A brief description of how the dependency is used.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "A list of dictionaries. Each dictionary should represent one dependency relationship and include the following keys:\n",
    "    - 'source': Name of the current module/file (string).\n",
    "    - 'target': The dependency module/file being referenced (string).\n",
    "    - 'type': The dependency type ('import', 'usage', etc.) (string).\n",
    "    - 'path': Full relative path to the target (string).\n",
    "    - 'external': `true` if the dependency is external, otherwise `false` (boolean).\n",
    "    - 'description': A short description of how the dependency is used (string).\n",
    "\"\"\"\n",
    "CODE_PARSER_PROMPT = \"\"\"\n",
    "You are a code analysis agent. Your task is to analyze the provided code and EXECUTE THE APPROPRIATE FUNCTIONS to register each code element you find:\n",
    "\n",
    "1. **Standalone Classes:** For EVERY standalone class you find (i.e., a class that is not nested inside another class or function):\n",
    "   - You MUST CALL the `extract_class_block` function with the class's docstring, description, name, and the class's complete code, including any methods and attributes defined within the class.\n",
    "   - This includes **all code related to the class**, such as methods, attributes, and any inner classes, if applicable.\n",
    "\n",
    "2. **Methods Inside Classes:** For EVERY method **inside a class**:\n",
    "   - You MUST CALL the `extract_method_block` function with the method's docstring, name, and code.\n",
    "   - Methods are always extracted within their corresponding class.\n",
    "\n",
    "3. **Standalone Functions:** For EVERY standalone function (i.e., functions that are not part of any class):\n",
    "   - You MUST CALL the `extract_method_block` function with the function's code, description, and the function name.\n",
    "   - This applies to functions that are **not inside any class** (top-level functions, not methods).\n",
    "\n",
    "4. **Standalone Scripts:** For EVERY script block that is **not part of a function or class** (global-level code, initialization code, script sections):\n",
    "   - You MUST CALL the `extract_script_block` function with the script's code and a description.\n",
    "   - This applies to **any code that exists outside of a function or class**.\n",
    "\n",
    "For each extracted element (class, function, or script), generate clear and concise description , ensuring that methods are nested within their corresponding classes, and script blocks are properly handled.\n",
    "\n",
    "Make sure to EXECUTE the appropriate function for each element:\n",
    "- `extract_class_block` for **standalone classes**, including all related code (methods, attributes, etc.).\n",
    "- `extract_method_block` for **methods** inside classes and **standalone functions**.\n",
    "- `extract_script_block` for **standalone script blocks** and **global code**.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_agent = FunctionAgent(\n",
    "    name=\"DescriptionAgent\",\n",
    "    description=\"Generates a detailed description of a file based on its code.\",\n",
    "    system_prompt=CODE_DESCRIPTION_PROMPT,    \n",
    "    llm=llm, \n",
    "    tools=[generate_file_description],\n",
    "\n",
    ")\n",
    "\n",
    "summary_agent = FunctionAgent(\n",
    "    name=\"SummaryAgent\",\n",
    "    description=\"Generates a summary of the code including its functionality.\",\n",
    "   system_prompt = CODE_SUMMARY_PROMPT,\n",
    "    llm=llm,\n",
    "    tools=[generate_code_summary],\n",
    ")\n",
    "\n",
    "\n",
    "# Complexity agent with proper handoff instructions\n",
    "complexity_agent = FunctionAgent(\n",
    "    name=\"ComplexityAgent\",\n",
    "    description=\"Analyzes the complexity of the code.\",\n",
    "    system_prompt=CODE_COMPLIXTY_PROMPT,\n",
    "    llm=llm,\n",
    "    tools=[analyze_complexity],\n",
    ")\n",
    "\n",
    "dependency_agent = FunctionAgent(\n",
    "    name=\"DependencyAgent\",\n",
    "    description=\"Analyzes code dependencies and extracts internal and external dependencies into a simplified structure.\",\n",
    "    system_prompt=CODE_DEPENDENCY_PROMPT,\n",
    "    llm=llm,\n",
    "    tools=[analyze_dependency],\n",
    ")\n",
    "\n",
    "# Define the parser agent responsible for analyzing code structure.\n",
    "parser_code_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    system_prompt = CODE_PARSER_PROMPT,\n",
    "    llm=llm,\n",
    "    tools_or_functions=[extract_class_block, extract_method_block, extract_script_block],\n",
    "    initial_state={\n",
    "            \"scripts\": [],\n",
    "            \"classes\": [],\n",
    "            \"methods\": []\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_base = \"./repos/sec-insights\"\n",
    "file_rel = \"backend/app/main.py\"  # for example\n",
    "# Extract file content asynchronously.\n",
    "\n",
    "file_content = await get_combined_file_content_with_tree(file_rel, repo_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity_result = await complexity_agent.run(file_content)\n",
    "# dependency_result = await dependency_agent.run(file_content)\n",
    "from llama_index.core.workflow import Context\n",
    "ctx = Context(parser_code_agent)\n",
    "file_content = await extract_file_content(file_rel, repo_base)\n",
    "complexity_result= await  complexity_agent.run(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The code exhibits moderate complexity. Here's a breakdown:\\n\\n**Algorithmic Complexity:**\\n\\n*   **`check_current_head`**: This function involves querying the database to get the current head revisions and comparing them to the Alembic script directory. The complexity depends on the database size and the number of migrations, but it's likely to be in the O(n) range, where n is the number of migrations.\\n*   **`__setup_logging`**: This function has a time complexity of O(1) as it performs a fixed set of operations.\\n*   **`__setup_sentry`**: The complexity is O(1) assuming `sentry_sdk.init` has constant time complexity.\\n*   **`lifespan`**: This function performs several operations:\\n    *   `check_database_connection`: The complexity depends on the implementation of this function, but it likely involves retries and timeouts, so it could take some time.\\n    *   Database operations for Alembic migrations: Similar to `check_current_head`, the complexity depends on the number of migrations.\\n    *   `get_vector_store_singleton` and `vector_store.run_setup()`:  The complexity here depends on the vector store initialization process, which could involve creating tables and indexes.\\n    *   `split_by_sentence_tokenizer()`: The first call might involve downloading NLTK files, which depends on network speed. Subsequent calls should be faster.\\n    *   `vector_store.close()`: The complexity depends on the vector store's cleanup process.\\n*   **`start`**:\\n    *   Alembic migrations (if `settings.RENDER` is true):  The complexity depends on the number of migrations.\\n    *   `uvicorn.run`: The complexity depends on the number of requests the application receives.\\n\\n**Code Structure and Readability:**\\n\\n*   The code is reasonably well-structured, with functions for different tasks (setting up logging, Sentry, database migrations, etc.).\\n*   The use of `asynccontextmanager` for the `lifespan` function is good for managing application startup and shutdown.\\n*   Type hints are used, which improves readability and helps with static analysis.\\n*   There are some long lines, especially in the CORS middleware configuration, which could be wrapped for better readability.\\n\\n**Potential Bottlenecks or Areas for Optimization:**\\n\\n*   **Database Migrations:** Running migrations on every startup (when `settings.RENDER` is true) can be a bottleneck, especially if there are many migrations. Consider alternative strategies for production deployments, such as running migrations as a separate step in the deployment pipeline.\\n*   **Vector Store Initialization:** The `vector_store.run_setup()` call in the `lifespan` function could be time-consuming.  Consider optimizing the vector store initialization process or performing it asynchronously.\\n*   **CORS Configuration:**  The `allow_origin_regex` for CORS could become a performance bottleneck if it becomes too complex. Consider more specific origin configurations if possible.\\n\\n**Overall Maintainability and Scalability:**\\n\\n*   The code is reasonably maintainable due to its structure and the use of type hints.\\n*   The use of configuration settings makes it easier to adapt the application to different environments.\\n*   The application should be scalable, as it uses FastAPI and Uvicorn, which are designed for asynchronous and concurrent request handling. However, the database and vector store could become bottlenecks as the application scales.\\n\\n**Impact on Performance and Future Development:**\\n\\n*   The database migrations on startup can slow down deployment times.\\n*   The vector store initialization can impact application startup time.\\n*   The CORS configuration could become a bottleneck if it becomes too complex.\\n*   Future development should focus on optimizing database queries, vector store operations, and the CORS configuration to ensure scalability and performance.\\n\\n**Recommendations:**\\n\\n*   Run database migrations as part of the deployment process instead of on every application startup in production environments.\\n*   Optimize the vector store initialization process.\\n*   Consider using a more specific CORS configuration instead of a complex regular expression.\\n*   Monitor the performance of the database and vector store as the application scales.\\n*   Add more logging and monitoring to help identify and diagnose performance issues.\\n\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(complexity_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await parser_code_agent.run(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_output_structures(agent_output):\n",
    "    state = {\n",
    "        \"classes\": [],\n",
    "        \"methods\": [],\n",
    "        \"scripts\": []\n",
    "    }\n",
    "\n",
    "    for tool_call in agent_output.tool_calls:\n",
    "        tool_name = tool_call.tool_name\n",
    "        kwargs = tool_call.tool_kwargs\n",
    "\n",
    "        if tool_name == \"extract_class_block\":\n",
    "            state[\"classes\"].append({\n",
    "                \"class_name\": kwargs.get(\"class_name\"),\n",
    "                \"description\": kwargs.get(\"description\"),\n",
    "                \"docstring\": kwargs.get(\"docstring\"),\n",
    "                \"code\": kwargs.get(\"code\")\n",
    "            })\n",
    "\n",
    "        elif tool_name == \"extract_method_block\":\n",
    "            state[\"methods\"].append({\n",
    "                \"method_name\": kwargs.get(\"method_name\"),\n",
    "                \"description\": kwargs.get(\"description\"),\n",
    "                \"docstring\": kwargs.get(\"docstring\", \"N/A\"),\n",
    "                \"code\": kwargs.get(\"code\")\n",
    "            })\n",
    "\n",
    "        elif tool_name == \"extract_script_block\":\n",
    "            state[\"scripts\"].append({\n",
    "                \"script_name\": kwargs.get(\"script_name\"),\n",
    "                \"description\": kwargs.get(\"description\"),\n",
    "                \"code\": kwargs.get(\"code\")\n",
    "            })\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [],\n",
       " 'methods': [{'method_name': 'check_current_head',\n",
       "   'description': 'Checks if the current database head matches the Alembic head.',\n",
       "   'docstring': None,\n",
       "   'code': 'def check_current_head(alembic_cfg: Config, connectable: Engine) -> bool:\\n    directory = script.ScriptDirectory.from_config(alembic_cfg)\\n    with connectable.begin() as connection:\\n        context = migration.MigrationContext.configure(connection)\\n        return set(context.get_current_heads()) == set(directory.get_heads())'},\n",
       "  {'method_name': '__setup_logging',\n",
       "   'description': 'Sets up logging configuration.',\n",
       "   'docstring': None,\n",
       "   'code': \"def __setup_logging(log_level: str):\\n    log_level = getattr(logging, log_level.upper())\\n    log_formatter = logging.Formatter(\\n        '%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s'\\n    )\\n    root_logger = logging.getLogger()\\n    root_logger.setLevel(log_level)\\n\\n    stream_handler = logging.StreamHandler(sys.stdout)\\n    stream_handler.setFormatter(log_formatter)\\n    root_logger.addHandler(stream_handler)\\n    logger.info('Set up logging with log level %s', log_level)\"},\n",
       "  {'method_name': '__setup_sentry',\n",
       "   'description': 'Sets up Sentry error tracking.',\n",
       "   'docstring': None,\n",
       "   'code': 'def __setup_sentry():\\n    if settings.SENTRY_DSN:\\n        logger.info(\"Setting up Sentry\")\\n        if settings.ENVIRONMENT == AppEnvironment.PRODUCTION:\\n            profiles_sample_rate = None\\n        else:\\n            profiles_sample_rate = settings.SENTRY_SAMPLE_RATE\\n        sentry_sdk.init(\\n            dsn=settings.SENTRY_DSN,\\n            environment=settings.ENVIRONMENT.value,\\n            release=settings.RENDER_GIT_COMMIT,\\n            debug=settings.VERBOSE,\\n            traces_sample_rate=settings.SENTRY_SAMPLE_RATE,\\n            profiles_sample_rate=profiles_sample_rate,\\n        )\\n    else:\\n        logger.info(\"Skipping Sentry setup\")'},\n",
       "  {'method_name': 'lifespan',\n",
       "   'description': \"Manages the application's lifecycle, including database connection, Alembic migrations, and vector store initialization.\",\n",
       "   'docstring': None,\n",
       "   'code': '@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    # first wait for DB to be connectable\\n    await check_database_connection()\\n    cfg = Config(\"alembic.ini\")\\n    # Change DB URL to use psycopg2 driver for this specific check\\n    db_url = settings.DATABASE_URL.replace(\\n        \"postgresql+asyncpg://\", \"postgresql+psycopg2://\"\\n    )\\n    cfg.set_main_option(\"sqlalchemy.url\", db_url)\\n    engine = create_engine(db_url, echo=True)\\n    if not check_current_head(cfg, engine):\\n        raise Exception(\\n            \"Database is not up to date. Please run `poetry run alembic upgrade head`\"\\n        )\\n    # initialize pg vector store singleton\\n    vector_store = await get_vector_store_singleton()\\n    vector_store = cast(CustomPGVectorStore, vector_store)\\n    await vector_store.run_setup()\\n\\n    try:\\n        # Some setup is required to initialize the llama-index sentence splitter\\n        split_by_sentence_tokenizer()\\n    except FileExistsError:\\n        # Sometimes seen in deployments, should be benign.\\n        logger.info(\"Tried to re-download NLTK files but already exists.\")\\n    yield\\n    # This section is run on app shutdown\\n    await vector_store.close()'},\n",
       "  {'method_name': 'start',\n",
       "   'description': 'Starts the FastAPI application with Uvicorn, configures logging and Sentry, and runs database migrations if necessary.',\n",
       "   'docstring': 'Launched with `poetry run start` at root level',\n",
       "   'code': 'def start():\\n    print(\"Running in AppEnvironment: \" + settings.ENVIRONMENT.value)\\n    __setup_logging(settings.LOG_LEVEL)\\n    __setup_sentry()\\n    \"\"\"Launched with `poetry run start` at root level\"\"\"\\n    if settings.RENDER:\\n        # on render.com deployments, run migrations\\n        logger.debug(\"Running migrations\")\\n        alembic_args = [\"--raiseerr\", \"upgrade\", \"head\"]\\n        alembic.config.main(argv=alembic_args)\\n        logger.debug(\"Migrations complete\")\\n    else:\\n        logger.debug(\"Skipping migrations\")\\n    live_reload = not settings.RENDER\\n    uvicorn.run(\\n        \"app.main:app\",\\n        host=\"0.0.0.0\",\\n        port=8000,\\n        reload=live_reload,\\n        workers=settings.UVICORN_WORKER_COUNT,\\n    )'}],\n",
       " 'scripts': [{'script_name': 'imports_and_logging',\n",
       "   'description': 'Imports necessary libraries and configures logging.',\n",
       "   'code': 'from typing import cast\\nimport uvicorn\\nimport logging\\nimport sys\\nimport sentry_sdk\\nfrom fastapi import FastAPI\\nfrom starlette.middleware.cors import CORSMiddleware\\nfrom alembic.config import Config\\nimport alembic.config\\nfrom alembic import script\\nfrom alembic.runtime import migration\\nfrom sqlalchemy.engine import create_engine, Engine\\nfrom llama_index.node_parser.text.utils import split_by_sentence_tokenizer\\n\\nfrom app.api.api import api_router\\nfrom app.db.wait_for_db import check_database_connection\\nfrom app.core.config import settings, AppEnvironment\\nfrom app.loader_io import loader_io_router\\nfrom contextlib import asynccontextmanager\\nfrom app.chat.pg_vector import get_vector_store_singleton, CustomPGVectorStore\\n\\nlogger = logging.getLogger(__name__)'},\n",
       "  {'script_name': 'fastapi_app_setup',\n",
       "   'description': 'Initializes the FastAPI application, configures CORS middleware, and includes API routers.',\n",
       "   'code': 'app = FastAPI(\\n    title=settings.PROJECT_NAME,\\n    openapi_url=f\"{settings.API_PREFIX}/openapi.json\",\\n    lifespan=lifespan,\\n)\\n\\n\\nif settings.BACKEND_CORS_ORIGINS:\\n    origins = settings.BACKEND_CORS_ORIGINS.copy()\\n    if settings.CODESPACES and settings.CODESPACE_NAME and \\\\\\n        settings.ENVIRONMENT == AppEnvironment.LOCAL:\\n        # add codespace origin if running in Github codespace\\n        origins.append(f\"https://{settings.CODESPACE_NAME}-3000.app.github.dev\")\\n    # allow all origins\\n    app.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=origins,\\n        allow_origin_regex=\"https://llama-app-frontend.*\\\\.vercel\\\\.app\",\\n        allow_credentials=True,\\n        allow_methods=[\"*\"],\\n        allow_headers=[\"*\"],\\n    )\\n\\napp.include_router(api_router, prefix=settings.API_PREFIX)\\napp.mount(f\"/{settings.LOADER_IO_VERIFICATION_STR}\", loader_io_router)'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = extract_tool_output_structures(output)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair \n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "\n",
    "async def run_code_analysis_agent(file_path: str, repo_base:str):\n",
    "    \"\"\"Run description and summary in parallel, then conditionally run further analysis agents based on the summary result.\"\"\"\n",
    "    try:\n",
    "        file_content = await  extract_file_content(file_path,repo_base)\n",
    "        # Run description and summary agents in parallel\n",
    "        description_result, summary_result = await asyncio.gather(\n",
    "            description_agent.run(file_content),\n",
    "            summary_agent.run(file_content)\n",
    "        )\n",
    "        summary_result =  json_repair.loads(summary_result.response.content)\n",
    "        # Check the summary result\n",
    "        if not summary_result.get(\"need_analysis\", False):\n",
    "            return {\n",
    "                \"file_description\": str(description_result),\n",
    "                \"code_summary\": summary_result.get(\"summary\",\"\"),\n",
    "                \"file_content\":file_content,\n",
    "                \"message\": \"No further analysis required (summary returned False).\"\n",
    "            }\n",
    "        project_tree =  get_project_tree_string(repo_base)\n",
    "        combined_content = (\n",
    "            \"Project Tree:\\n\"\n",
    "            \"-------------\\n\"\n",
    "            f\"{project_tree}\\n\\n\"\n",
    "            \"File Content:\\n\"\n",
    "            \"-------------\\n\"\n",
    "            f\"{file_content}\"\n",
    "        )\n",
    "        ctx = Context(parser_code_agent)\n",
    "        await ctx.set(\"state\", {\n",
    "                \"classes\": [],\n",
    "                \"methods\": [],\n",
    "                \"scripts\": []\n",
    "            })\n",
    "        # If summary is True, continue with the other agents (complexity, dependency, parser)\n",
    "        complexity_result, dependency_result ,parser_code_result = await asyncio.gather(\n",
    "            complexity_agent.run(file_content),\n",
    "            dependency_agent.run(combined_content),\n",
    "            parser_code_agent.run(file_content,ctx=ctx)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Aggregate the results into the state dictionary\n",
    "        state = {\n",
    "            \"file_description\": str(description_result),\n",
    "            \"code_summary\": summary_result.get(\"summary\",\"\"),\n",
    "            \"complexity_analysis\": complexity_result.response.content,\n",
    "            \"dependency_analysis\":json_repair.loads(dependency_result.response.content),\n",
    "            \"code_analysis\": parser_code_result,\n",
    "            \"file_content\":file_content\n",
    "        }\n",
    "\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code analysis: {e}\")\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = await get_combined_file_content_with_tree(file_rel, repo_base)\n",
    "repo_base = \"./repos/sec-insights\"\n",
    "file_rel = \"backend/app/main.py\"  # for example\n",
    "states = await run_code_analysis_agent(file_rel, repo_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[\"code_analysis\"].get(\"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text=\"I have analyzed the code and extracted the following elements:\\n\\n- **Script:** Imports necessary libraries and configures logging.\\n- **Function:** `check_current_head`: Checks if the current database head matches the Alembic head.\\n- **Function:** `__setup_logging`: Sets up logging configuration.\\n- **Function:** `__setup_sentry`: Sets up Sentry error tracking.\\n- **Function:** `lifespan`: Manages the application's lifecycle, including database connection, Alembic migrations, and vector store initialization.\\n- **Script:** Initializes the FastAPI application, configures CORS middleware, and includes API routers.\\n- **Function:** `start`: Starts the FastAPI application with Uvicorn, configures logging and Sentry, and runs database migrations if necessary.\\n\")]), tool_calls=[ToolCallResult(tool_name='extract_method_block', tool_kwargs={'description': 'Checks if the current database head matches the Alembic head.', 'docstring': None, 'code': '\\ndef check_current_head(alembic_cfg: Config, connectable: Engine) -> bool:\\n    directory = script.ScriptDirectory.from_config(alembic_cfg)\\n    with connectable.begin() as connection:\\n        context = migration.MigrationContext.configure(connection)\\n        return set(context.get_current_heads()) == set(directory.get_heads())\\n', 'method_name': 'check_current_head'}, tool_id='extract_method_block', tool_output=ToolOutput(content='Method block extraction completed', tool_name='extract_method_block', raw_input={'args': (), 'kwargs': {'description': 'Checks if the current database head matches the Alembic head.', 'docstring': None, 'code': '\\ndef check_current_head(alembic_cfg: Config, connectable: Engine) -> bool:\\n    directory = script.ScriptDirectory.from_config(alembic_cfg)\\n    with connectable.begin() as connection:\\n        context = migration.MigrationContext.configure(connection)\\n        return set(context.get_current_heads()) == set(directory.get_heads())\\n', 'method_name': 'check_current_head'}}, raw_output='Method block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_method_block', tool_kwargs={'code': '\\ndef __setup_logging(log_level: str):\\n    log_level = getattr(logging, log_level.upper())\\n    log_formatter = logging.Formatter(\\n        \"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\"\\n    )\\n    root_logger = logging.getLogger()\\n    root_logger.setLevel(log_level)\\n\\n    stream_handler = logging.StreamHandler(sys.stdout)\\n    stream_handler.setFormatter(log_formatter)\\n    root_logger.addHandler(stream_handler)\\n    logger.info(\"Set up logging with log level %s\", log_level)\\n', 'method_name': '__setup_logging', 'description': 'Sets up logging configuration.', 'docstring': None}, tool_id='extract_method_block', tool_output=ToolOutput(content='Method block extraction completed', tool_name='extract_method_block', raw_input={'args': (), 'kwargs': {'code': '\\ndef __setup_logging(log_level: str):\\n    log_level = getattr(logging, log_level.upper())\\n    log_formatter = logging.Formatter(\\n        \"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\"\\n    )\\n    root_logger = logging.getLogger()\\n    root_logger.setLevel(log_level)\\n\\n    stream_handler = logging.StreamHandler(sys.stdout)\\n    stream_handler.setFormatter(log_formatter)\\n    root_logger.addHandler(stream_handler)\\n    logger.info(\"Set up logging with log level %s\", log_level)\\n', 'method_name': '__setup_logging', 'description': 'Sets up logging configuration.', 'docstring': None}}, raw_output='Method block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_method_block', tool_kwargs={'docstring': None, 'method_name': '__setup_sentry', 'description': 'Sets up Sentry error tracking.', 'code': '\\ndef __setup_sentry():\\n    if settings.SENTRY_DSN:\\n        logger.info(\"Setting up Sentry\")\\n        if settings.ENVIRONMENT == AppEnvironment.PRODUCTION:\\n            profiles_sample_rate = None\\n        else:\\n            profiles_sample_rate = settings.SENTRY_SAMPLE_RATE\\n        sentry_sdk.init(\\n            dsn=settings.SENTRY_DSN,\\n            environment=settings.ENVIRONMENT.value,\\n            release=settings.RENDER_GIT_COMMIT,\\n            debug=settings.VERBOSE,\\n            traces_sample_rate=settings.SENTRY_SAMPLE_RATE,\\n            profiles_sample_rate=profiles_sample_rate,\\n        )\\n    else:\\n        logger.info(\"Skipping Sentry setup\")\\n'}, tool_id='extract_method_block', tool_output=ToolOutput(content='Method block extraction completed', tool_name='extract_method_block', raw_input={'args': (), 'kwargs': {'docstring': None, 'method_name': '__setup_sentry', 'description': 'Sets up Sentry error tracking.', 'code': '\\ndef __setup_sentry():\\n    if settings.SENTRY_DSN:\\n        logger.info(\"Setting up Sentry\")\\n        if settings.ENVIRONMENT == AppEnvironment.PRODUCTION:\\n            profiles_sample_rate = None\\n        else:\\n            profiles_sample_rate = settings.SENTRY_SAMPLE_RATE\\n        sentry_sdk.init(\\n            dsn=settings.SENTRY_DSN,\\n            environment=settings.ENVIRONMENT.value,\\n            release=settings.RENDER_GIT_COMMIT,\\n            debug=settings.VERBOSE,\\n            traces_sample_rate=settings.SENTRY_SAMPLE_RATE,\\n            profiles_sample_rate=profiles_sample_rate,\\n        )\\n    else:\\n        logger.info(\"Skipping Sentry setup\")\\n'}}, raw_output='Method block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_method_block', tool_kwargs={'code': '\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    # first wait for DB to be connectable\\n    await check_database_connection()\\n    cfg = Config(\"alembic.ini\")\\n    # Change DB URL to use psycopg2 driver for this specific check\\n    db_url = settings.DATABASE_URL.replace(\\n        \"postgresql+asyncpg://\", \"postgresql+psycopg2://\"\\n    )\\n    cfg.set_main_option(\"sqlalchemy.url\", db_url)\\n    engine = create_engine(db_url, echo=True)\\n    if not check_current_head(cfg, engine):\\n        raise Exception(\\n            \"Database is not up to date. Please run `poetry run alembic upgrade head`\"\\n        )\\n    # initialize pg vector store singleton\\n    vector_store = await get_vector_store_singleton()\\n    vector_store = cast(CustomPGVectorStore, vector_store)\\n    await vector_store.run_setup()\\n\\n    try:\\n        # Some setup is required to initialize the llama-index sentence splitter\\n        split_by_sentence_tokenizer()\\n    except FileExistsError:\\n        # Sometimes seen in deployments, should be benign.\\n        logger.info(\"Tried to re-download NLTK files but already exists.\")\\n    yield\\n    # This section is run on app shutdown\\n    await vector_store.close()\\n', 'method_name': 'lifespan', 'docstring': None, 'description': \"Manages the application's lifecycle, including database connection, Alembic migrations, and vector store initialization.\"}, tool_id='extract_method_block', tool_output=ToolOutput(content='Method block extraction completed', tool_name='extract_method_block', raw_input={'args': (), 'kwargs': {'code': '\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    # first wait for DB to be connectable\\n    await check_database_connection()\\n    cfg = Config(\"alembic.ini\")\\n    # Change DB URL to use psycopg2 driver for this specific check\\n    db_url = settings.DATABASE_URL.replace(\\n        \"postgresql+asyncpg://\", \"postgresql+psycopg2://\"\\n    )\\n    cfg.set_main_option(\"sqlalchemy.url\", db_url)\\n    engine = create_engine(db_url, echo=True)\\n    if not check_current_head(cfg, engine):\\n        raise Exception(\\n            \"Database is not up to date. Please run `poetry run alembic upgrade head`\"\\n        )\\n    # initialize pg vector store singleton\\n    vector_store = await get_vector_store_singleton()\\n    vector_store = cast(CustomPGVectorStore, vector_store)\\n    await vector_store.run_setup()\\n\\n    try:\\n        # Some setup is required to initialize the llama-index sentence splitter\\n        split_by_sentence_tokenizer()\\n    except FileExistsError:\\n        # Sometimes seen in deployments, should be benign.\\n        logger.info(\"Tried to re-download NLTK files but already exists.\")\\n    yield\\n    # This section is run on app shutdown\\n    await vector_store.close()\\n', 'method_name': 'lifespan', 'docstring': None, 'description': \"Manages the application's lifecycle, including database connection, Alembic migrations, and vector store initialization.\"}}, raw_output='Method block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_method_block', tool_kwargs={'method_name': 'start', 'docstring': 'Launched with `poetry run start` at root level', 'description': 'Starts the FastAPI application with Uvicorn, configures logging and Sentry, and runs database migrations if necessary.', 'code': '\\ndef start():\\n    print(\"Running in AppEnvironment: \" + settings.ENVIRONMENT.value)\\n    __setup_logging(settings.LOG_LEVEL)\\n    __setup_sentry()\\n    \"\"\"Launched with `poetry run start` at root level\"\"\"\\n    if settings.RENDER:\\n        # on render.com deployments, run migrations\\n        logger.debug(\"Running migrations\")\\n        alembic_args = [\"--raiseerr\", \"upgrade\", \"head\"]\\n        alembic.config.main(argv=alembic_args)\\n        logger.debug(\"Migrations complete\")\\n    else:\\n        logger.debug(\"Skipping migrations\")\\n    live_reload = not settings.RENDER\\n    uvicorn.run(\\n        \"app.main:app\",\\n        host=\"0.0.0.0\",\\n        port=8000,\\n        reload=live_reload,\\n        workers=settings.UVICORN_WORKER_COUNT,\\n    )\\n'}, tool_id='extract_method_block', tool_output=ToolOutput(content='Method block extraction completed', tool_name='extract_method_block', raw_input={'args': (), 'kwargs': {'method_name': 'start', 'docstring': 'Launched with `poetry run start` at root level', 'description': 'Starts the FastAPI application with Uvicorn, configures logging and Sentry, and runs database migrations if necessary.', 'code': '\\ndef start():\\n    print(\"Running in AppEnvironment: \" + settings.ENVIRONMENT.value)\\n    __setup_logging(settings.LOG_LEVEL)\\n    __setup_sentry()\\n    \"\"\"Launched with `poetry run start` at root level\"\"\"\\n    if settings.RENDER:\\n        # on render.com deployments, run migrations\\n        logger.debug(\"Running migrations\")\\n        alembic_args = [\"--raiseerr\", \"upgrade\", \"head\"]\\n        alembic.config.main(argv=alembic_args)\\n        logger.debug(\"Migrations complete\")\\n    else:\\n        logger.debug(\"Skipping migrations\")\\n    live_reload = not settings.RENDER\\n    uvicorn.run(\\n        \"app.main:app\",\\n        host=\"0.0.0.0\",\\n        port=8000,\\n        reload=live_reload,\\n        workers=settings.UVICORN_WORKER_COUNT,\\n    )\\n'}}, raw_output='Method block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_script_block', tool_kwargs={'description': 'Initializes the FastAPI application, configures CORS middleware, and includes API routers.', 'code': '\\napp = FastAPI(\\n    title=settings.PROJECT_NAME,\\n    openapi_url=f\"{settings.API_PREFIX}/openapi.json\",\\n    lifespan=lifespan,\\n)\\n\\n\\nif settings.BACKEND_CORS_ORIGINS:\\n    origins = settings.BACKEND_CORS_ORIGINS.copy()\\n    if settings.CODESPACES and settings.CODESPACE_NAME and \\\\\\n        settings.ENVIRONMENT == AppEnvironment.LOCAL:\\n        # add codespace origin if running in Github codespace\\n        origins.append(f\"https://{settings.CODESPACE_NAME}-3000.app.github.dev\")\\n    # allow all origins\\n    app.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=origins,\\n        allow_origin_regex=\"https://llama-app-frontend.*\\\\.vercel\\\\.app\",\\n        allow_credentials=True,\\n        allow_methods=[\"*\"],\\n        allow_headers=[\"*\"],\\n    )\\n\\napp.include_router(api_router, prefix=settings.API_PREFIX)\\napp.mount(f\"/{settings.LOADER_IO_VERIFICATION_STR}\", loader_io_router)\\n', 'script_name': 'fastapi_app_setup'}, tool_id='extract_script_block', tool_output=ToolOutput(content='Script block extraction completed', tool_name='extract_script_block', raw_input={'args': (), 'kwargs': {'description': 'Initializes the FastAPI application, configures CORS middleware, and includes API routers.', 'code': '\\napp = FastAPI(\\n    title=settings.PROJECT_NAME,\\n    openapi_url=f\"{settings.API_PREFIX}/openapi.json\",\\n    lifespan=lifespan,\\n)\\n\\n\\nif settings.BACKEND_CORS_ORIGINS:\\n    origins = settings.BACKEND_CORS_ORIGINS.copy()\\n    if settings.CODESPACES and settings.CODESPACE_NAME and \\\\\\n        settings.ENVIRONMENT == AppEnvironment.LOCAL:\\n        # add codespace origin if running in Github codespace\\n        origins.append(f\"https://{settings.CODESPACE_NAME}-3000.app.github.dev\")\\n    # allow all origins\\n    app.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=origins,\\n        allow_origin_regex=\"https://llama-app-frontend.*\\\\.vercel\\\\.app\",\\n        allow_credentials=True,\\n        allow_methods=[\"*\"],\\n        allow_headers=[\"*\"],\\n    )\\n\\napp.include_router(api_router, prefix=settings.API_PREFIX)\\napp.mount(f\"/{settings.LOADER_IO_VERIFICATION_STR}\", loader_io_router)\\n', 'script_name': 'fastapi_app_setup'}}, raw_output='Script block extraction completed', is_error=False), return_direct=False), ToolCallResult(tool_name='extract_script_block', tool_kwargs={'code': 'from typing import cast\\nimport uvicorn\\nimport logging\\nimport sys\\nimport sentry_sdk\\nfrom fastapi import FastAPI\\nfrom starlette.middleware.cors import CORSMiddleware\\nfrom alembic.config import Config\\nimport alembic.config\\nfrom alembic import script\\nfrom alembic.runtime import migration\\nfrom sqlalchemy.engine import create_engine, Engine\\nfrom llama_index.node_parser.text.utils import split_by_sentence_tokenizer\\n\\nfrom app.api.api import api_router\\nfrom app.db.wait_for_db import check_database_connection\\nfrom app.core.config import settings, AppEnvironment\\nfrom app.loader_io import loader_io_router\\nfrom contextlib import asynccontextmanager\\nfrom app.chat.pg_vector import get_vector_store_singleton, CustomPGVectorStore\\n\\nlogger = logging.getLogger(__name__)\\n', 'script_name': 'imports_and_logging', 'description': 'Imports necessary libraries and configures logging.'}, tool_id='extract_script_block', tool_output=ToolOutput(content='Script block extraction completed', tool_name='extract_script_block', raw_input={'args': (), 'kwargs': {'code': 'from typing import cast\\nimport uvicorn\\nimport logging\\nimport sys\\nimport sentry_sdk\\nfrom fastapi import FastAPI\\nfrom starlette.middleware.cors import CORSMiddleware\\nfrom alembic.config import Config\\nimport alembic.config\\nfrom alembic import script\\nfrom alembic.runtime import migration\\nfrom sqlalchemy.engine import create_engine, Engine\\nfrom llama_index.node_parser.text.utils import split_by_sentence_tokenizer\\n\\nfrom app.api.api import api_router\\nfrom app.db.wait_for_db import check_database_connection\\nfrom app.core.config import settings, AppEnvironment\\nfrom app.loader_io import loader_io_router\\nfrom contextlib import asynccontextmanager\\nfrom app.chat.pg_vector import get_vector_store_singleton, CustomPGVectorStore\\n\\nlogger = logging.getLogger(__name__)\\n', 'script_name': 'imports_and_logging', 'description': 'Imports necessary libraries and configures logging.'}}, raw_output='Script block extraction completed', is_error=False), return_direct=False)], raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': ', configures logging and Sentry, and runs database migrations if necessary.\\n'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 162, 'prompt_token_count': 3647, 'total_token_count': 3809}}, current_agent_name='Agent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
