{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich KG Agent using MutliAgent WorkFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-google-genai llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiofiles\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow\n",
    "from llama_index.core.agent.workflow import AgentInput, AgentOutput, ToolCall, ToolCallResult, AgentStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"]=\"AIzaSyBuGAPWnqtxGoCBnSgF_jm8X74-0CSavsk\"\n",
    "llm = GoogleGenAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----- TOOL FUNCTIONS -----\n",
    "\n",
    "import os\n",
    "import aiofiles\n",
    "\n",
    "async def extract_file_content(file_relative_path: str, repo_base_path: str) -> str:\n",
    "    \"\"\"Extracts and returns the content of a file given its relative path from the repository base asynchronously.\"\"\"\n",
    "    absolute_path = os.path.join(repo_base_path, file_relative_path)\n",
    "    try:\n",
    "        async with aiofiles.open(absolute_path, mode='r', encoding='utf-8') as f:\n",
    "            content = await f.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            async with aiofiles.open(absolute_path, mode='r', encoding='latin-1') as f:\n",
    "                content = await f.read()\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file (tried multiple encodings): {str(e)}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"File not found: {absolute_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "def get_project_tree_string(root_path: str, prefix: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Recursively generates a tree-like string for the given directory.\n",
    "    Example output:\n",
    "        ├── folder1\n",
    "        │   ├── file1.py\n",
    "        │   └── file2.py\n",
    "        └── folder2\n",
    "            └── file3.py\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    try:\n",
    "        entries = os.listdir(root_path)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading directory {root_path}: {e}\"\n",
    "    \n",
    "    entries.sort()\n",
    "    entries_count = len(entries)\n",
    "    for index, entry in enumerate(entries):\n",
    "        full_path = os.path.join(root_path, entry)\n",
    "        is_last = (index == entries_count - 1)\n",
    "        connector = \"└── \" if is_last else \"├── \"\n",
    "        lines.append(prefix + connector + entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            extension_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            subtree = get_project_tree_string(full_path, extension_prefix)\n",
    "            if subtree:\n",
    "                lines.append(subtree)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def get_combined_file_content_with_tree(file_relative_path: str, repo_base_path: str) -> str:\n",
    "    \"\"\"Combines the project tree (as textual context) with the content of a file.\"\"\"\n",
    "    file_content = await extract_file_content(file_relative_path, repo_base_path)    \n",
    "    project_tree = get_project_tree_string(repo_base_path)\n",
    "    combined_content = (\n",
    "        \"Project Tree:\\n\"\n",
    "        \"-------------\\n\"\n",
    "        f\"{project_tree}\\n\\n\"\n",
    "        \"File Content:\\n\"\n",
    "        \"-------------\\n\"\n",
    "        f\"{file_content}\"\n",
    "    )\n",
    "    return combined_content\n",
    "\n",
    "\n",
    "async def generate_file_description(ctx: Context, description:str) ->str :\n",
    "    \"\"\"Usefull to generate detailed description of a file based on its code\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"file_description\"] = description\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Description recorded.\"\n",
    "\n",
    "async def generate_code_summary(ctx: Context, summary: str, need_analysis: bool) -> str:\n",
    "    \"\"\"Generates a detailed summary of a file's code and determines if further analysis is necessary based on the file's content.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"code_summary\" not in current_state:\n",
    "        current_state[\"code_summary\"] = {}\n",
    "    current_state[\"code_summary\"][\"code_summary\"] = summary\n",
    "    current_state[\"code_summary\"][\"need_analysis\"] =  need_analysis\n",
    "    await ctx.set(\"state\", current_state)    \n",
    "    return \"Summary recorded.\"\n",
    "\n",
    "\n",
    "async def analyze_complexity(ctx: Context, complexity_analysis: str) -> str:\n",
    "    \"\"\"Usefull to generate detailed analyze complexity of a file based on its code\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"complexity_analysis\"] = complexity_analysis\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"complexity analysis detailed.\"\n",
    "\n",
    "async def analyze_dependency(\n",
    "    ctx: Context,\n",
    "    source: str,\n",
    "    target: str,\n",
    "    full_path:str,\n",
    "    type: str,\n",
    "    external: bool,\n",
    "    description: str\n",
    ") -> str:\n",
    "    \"\"\"Analyzes code dependencies and records detailed dependency information as a structured list\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"dependency_analysis\" not in current_state:\n",
    "        current_state[\"dependency_analysis\"] = []\n",
    "    \n",
    "    dependency_item = {\n",
    "        \"source\": source,\n",
    "        \"target\": target,\n",
    "        \"full_path\":full_path,\n",
    "        \"type\": type,\n",
    "        \"external\": external,\n",
    "        \"description\":description\n",
    "    }\n",
    "    \n",
    "    current_state[\"dependency_analysis\"].append(dependency_item)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Dependency analysis completed with structured data.\"\n",
    "\n",
    "async def extract_class_block(\n",
    "        ctx: Context,\n",
    "        docstring: str,\n",
    "        class_name:str,\n",
    "        code: str,\n",
    "        description: str\n",
    "    ):\n",
    "    \"\"\"Usefull to Extracts and registers a class block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"code_analysis\" not in current_state:\n",
    "        current_state[\"code_analysis\"] = {\"classes\": []}\n",
    "    class_block = {\n",
    "        \"description\": description,\n",
    "        \"docstring\": docstring,\n",
    "        \"class\":class_name,\n",
    "        \"code\": code\n",
    "    }\n",
    "    current_state[\"code_analysis\"][\"classes\"].append(class_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Class block extraction completed\"\n",
    "\n",
    "\n",
    "async def extract_method_block(\n",
    "    ctx: Context,\n",
    "    docstring: str,\n",
    "    method_name: str,\n",
    "    code: str,\n",
    "    description: str\n",
    "):\n",
    "    \"\"\"Usefull to Extracts and registers a method block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"code_analysis\" not in current_state:\n",
    "        current_state[\"code_analysis\"] = {\"methods\": []}\n",
    "    method_block = {\n",
    "        \"description\": description,\n",
    "        \"docstring\": docstring,\n",
    "        \"method\": method_name,\n",
    "        \"code\": code\n",
    "    }\n",
    "    current_state[\"code_analysis\"][\"methods\"].append(method_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Method block extraction completed\"\n",
    "\n",
    "\n",
    "async def extract_script_block(\n",
    "    ctx: Context,\n",
    "    code: str,\n",
    "    description: str,\n",
    "    script_name:str,\n",
    "):\n",
    "    \"\"\"Usefull to Extracts and registers a script block from the provided code.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"code_analysis\" not in current_state:\n",
    "        current_state[\"code_analysis\"] = {\"scripts\": []}\n",
    "    script_block = {\n",
    "        \"script_name\":script_name,\n",
    "        \"description\": description,\n",
    "        \"code\": code,\n",
    "    }\n",
    "    current_state[\"code_analysis\"][\"scripts\"].append(script_block)\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Script block extraction completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "description_agent = FunctionAgent(\n",
    "    name=\"DescriptionAgent\",\n",
    "    description=\"Generates a detailed description of a file based on its code.\",\n",
    "    system_prompt=(\n",
    "        \"Please provide a clear, professional, and comprehensive description of the file's purpose and functionality based solely on the provided code.\"\n",
    "        \"After generated the Description you should hand off control to  SummaryAgent to genearte code summary.\"\n",
    "        ),    \n",
    "    llm=llm,  # Replace with your LLM instance as needed.\n",
    "    tools=[generate_file_description],\n",
    "    can_handoff_to=[\"SummaryAgent\"],\n",
    "\n",
    ")\n",
    "\n",
    "summary_agent = FunctionAgent(\n",
    "    name=\"SummaryAgent\",\n",
    "    description=\"Generates a summary of the code including its functionality.\",\n",
    "   system_prompt = \"\"\"\n",
    "    Provide a high-level overview of what the code does, outlining its key functionality, main components, \n",
    "    and purpose within the larger system. The summary should capture the essence of the code without delving into \n",
    "    implementation details.\n",
    "\n",
    "    After generating the summary, return a boolean indicating whether this file requires further analysis. \n",
    "    - If Return `True` You should hand of control to  ComplexityAgent for detailed analysis. \n",
    "    - If Return `False` should be skipped and no further analysis is needed (e.g., for Dockerfiles, README.md, .sh files).\n",
    "    \"\"\",\n",
    "    llm=llm,\n",
    "    tools=[generate_code_summary],\n",
    "    can_handoff_to=[\"ComplexityAgent\"],\n",
    "\n",
    ")\n",
    "# Complexity agent with proper handoff instructions\n",
    "complexity_agent = FunctionAgent(\n",
    "    name=\"ComplexityAgent\",\n",
    "    description=\"Analyzes the complexity of the code.\",\n",
    "    system_prompt=(\n",
    "        \"Analyze the code's complexity and provide a comprehensive assessment.\\n\\n\"\n",
    "        \"After completing the complexity analysis, hand off control to the ParserCodeAgent \"\n",
    "        \"to perform a  code  parsering and analysis\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[analyze_complexity],\n",
    "    can_handoff_to=[\"ParserCodeAgent\"],\n",
    ")\n",
    "\n",
    "dependency_agent = FunctionAgent(\n",
    "    name=\"DependencyAgent\",\n",
    "    description=\"Analyzes code dependencies and extracts internal and external dependencies into a simplified structure.\",\n",
    "    system_prompt=\"\"\"\n",
    "    Your task is to analyze the provided code and extract dependencies into a simple format that is easy to load into a Neo4j database.\n",
    "\n",
    "    ANALYSIS PROCEDURE:\n",
    "    1. Identify all imports (standard libraries, third-party packages, local modules).\n",
    "    2. Detect function or class dependencies within the file.\n",
    "    3. Find references to other project files and external libraries.\n",
    "    4. For local dependencies, resolve import paths using the provided project tree, converting dotted paths into full relative paths.\n",
    "    5. For each dependency, create a structured output that includes:\n",
    "        - 'source': The file where the dependency originates.\n",
    "        - 'target': The file/module being referenced.\n",
    "        - 'type': The type of dependency (e.g., 'import', 'usage', etc.).\n",
    "        - 'path': The full relative path to the dependency (e.g., `app/db/wait_for_db.py`).\n",
    "        - 'external': Whether the dependency is external (e.g., third-party packages like `requests`) or internal (use `true/false`).\n",
    "        - 'description': A brief description of how the dependency is used.\n",
    "\n",
    "    OUTPUT FORMAT:\n",
    "    A list of dictionaries. Each dictionary should represent one dependency relationship and include the following keys:\n",
    "        - 'source': Name of the current module/file (string).\n",
    "        - 'target': The dependency module/file being referenced (string).\n",
    "        - 'type': The dependency type ('import', 'usage', etc.) (string).\n",
    "        - 'path': Full relative path to the target (string).\n",
    "        - 'external': `true` if the dependency is external, otherwise `false` (boolean).\n",
    "        - 'description': A short description of how the dependency is used (string).\n",
    "    \"\"\",\n",
    "    llm=llm,\n",
    "    tools=[analyze_dependency],\n",
    ")\n",
    "\n",
    "# Define the parser agent responsible for analyzing code structure.\n",
    "parser_code_agent = FunctionAgent(\n",
    "    name=\"ParserCodeAgent\",\n",
    "    description=(\n",
    "        \"This agent is responsible for parsing code  and generate documentation to extract class definitions, \"\n",
    "        \"methods, and script blocks. For each class, the agent aggregates its methods \"\n",
    "        \"and associated script blocks, while generating descriptive documentation \"\n",
    "    ),\n",
    "    system_prompt=(\"\"\"\n",
    "    You are a code analysis agent. Your task is to analyze the given code and accurately identify and register and generate the description:\n",
    "\n",
    "    1. Class Blocks: Extract complete class definitions, including their docstrings and code. For each class found, use the extract_class_block function. \n",
    "    2. Method Blocks: Within classes, identify methods along with their names, docstrings, and code. For each method, use the extract_method_block function.\n",
    "    3. Script Blocks: Identify any standalone code blocks (i.e., not part of a class or method) and extract them as script blocks using the extract_script_block function.\n",
    "\n",
    "    For each extracted entity, generate clear, concise documentation and ensure that methods are nested or associated with their corresponding class where applicable. Make sure to always use the correct extraction function based on whether the block of code is a class, method, or a standalone script    \n",
    "    After completing the complexity analysis, hand off control to the DependencyAgent \n",
    "    to perform a detailed code dependency analysis. The DependencyAgent will extract\n",
    "    dependencies in a structured format suitable for Neo4j integration.\n",
    "                   \n",
    "    \"\"\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[extract_class_block, extract_method_block, extract_script_block],\n",
    "    can_handoff_to=[\"DependencyAgent\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the workflow with an initial state.\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[\n",
    "        description_agent,\n",
    "        summary_agent,\n",
    "        complexity_agent,\n",
    "        dependency_agent,\n",
    "        parser_code_agent\n",
    "    ],\n",
    "    root_agent=description_agent.name,\n",
    "    initial_state={\n",
    "        \"file_description\": \"\",\n",
    "        \"code_summary\": {},\n",
    "        \"complexity_analysis\": \"\",\n",
    "        \"dependency_analysis\": [],\n",
    "        \"code_analysis\": {\n",
    "            \"scripts\": [],\n",
    "            \"classes\": [],\n",
    "            \"methods\": []\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This file serves as the entry point for the backend application, configuring and launching the FastAPI application. It manages the application lifecycle, including database initialization and migrations, sets up middleware for CORS, configures logging and error tracking with Sentry, and includes routers for different API endpoints. The application is then run using Uvicorn.\\n\\nI will now hand off control to the SummaryAgent to generate a code summary.\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "repo_base = \"./repos/sec-insights\"\n",
    "file_rel = \"backend/app/main.py\"  # for example\n",
    "# Extract file content asynchronously.\n",
    "file_content = await get_combined_file_content_with_tree(file_rel, repo_base)\n",
    "\n",
    "result = await description_agent.run(file_content)\n",
    "\n",
    "result.response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🤖 Agent: DescriptionAgent\n",
      "==================================================\n",
      "\n",
      "This file serves as the entry point for the FastAPI application. It handles the configuration and initialization of various components, including logging, error tracking with Sentry, CORS settings, database connection and migration checks using Alembic, and the lifespan of the application. The lifespan function ensures the database is up-to-date and initializes a vector store. It also sets up API routes and starts the Uvicorn server to run the application.\n",
      "\n",
      "📤 Output: This file serves as the entry point for the FastAPI application. It handles the configuration and initialization of various components, including logging, error tracking with Sentry, CORS settings, database connection and migration checks using Alembic, and the lifespan of the application. The lifespan function ensures the database is up-to-date and initializes a vector store. It also sets up API routes and starts the Uvicorn server to run the application.\n",
      "\n",
      "\n",
      "🛠️  Planning to use tools: ['handoff']\n",
      "🔨 Calling Tool: handoff\n",
      "  With arguments: {'to_agent': 'SummaryAgent', 'reason': 'Now that the file description is generated, handoff to the SummaryAgent to generate a code summary.'}\n",
      "🔧 Tool Result (handoff):\n",
      "  Arguments: {'to_agent': 'SummaryAgent', 'reason': 'Now that the file description is generated, handoff to the SummaryAgent to generate a code summary.'}\n",
      "  Output: Agent SummaryAgent is now handling the request due to the following reason: Now that the file description is generated, handoff to the SummaryAgent to generate a code summary..\n",
      "Please continue with the current request.\n",
      "\n",
      "==================================================\n",
      "🤖 Agent: SummaryAgent\n",
      "==================================================\n",
      "\n",
      "🛠️  Planning to use tools: ['generate_code_summary']\n",
      "🔨 Calling Tool: generate_code_summary\n",
      "  With arguments: {'summary': 'This file defines the main FastAPI application. It sets up logging and error tracking with Sentry, configures CORS to allow requests from the frontend, and defines a lifespan function that handles database connection and migration checks, as well as initializing a vector store. It includes API routes and starts the Uvicorn server.', 'need_analysis': True}\n",
      "🔧 Tool Result (generate_code_summary):\n",
      "  Arguments: {'summary': 'This file defines the main FastAPI application. It sets up logging and error tracking with Sentry, configures CORS to allow requests from the frontend, and defines a lifespan function that handles database connection and migration checks, as well as initializing a vector store. It includes API routes and starts the Uvicorn server.', 'need_analysis': True}\n",
      "  Output: Summary recorded.\n",
      "The code summary has been generated and recorded. The file requires further analysis.\n",
      "📤 Output: The code summary has been generated and recorded. The file requires further analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----- Main Asynchronous Function -----\n",
    "\n",
    "repo_base = \"./repos/sec-insights\"\n",
    "file_rel = \"backend/app/main.py\"  # for example\n",
    "# Extract file content asynchronously.\n",
    "file_content = await get_combined_file_content_with_tree(file_rel, repo_base)\n",
    "\n",
    "# Start the agent workflow with the file content as input.\n",
    "handler = agent_workflow.run(user_msg=file_content)\n",
    "\n",
    "# Stream workflow events to monitor progress.\n",
    "current_agent = None\n",
    "async for event in handler.stream_events():\n",
    "    if hasattr(event, \"current_agent_name\") and event.current_agent_name != current_agent:\n",
    "        current_agent = event.current_agent_name\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"🤖 Agent: {current_agent}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "    if isinstance(event, AgentStream):\n",
    "        if event.delta:\n",
    "            print(event.delta, end=\"\", flush=True)\n",
    "    # elif isinstance(event, AgentInput):\n",
    "    #     print(\"📥 Input:\", event.input)        \n",
    "    if isinstance(event, AgentOutput):\n",
    "        if event.response.content:\n",
    "            print(\"📤 Output:\", event.response.content)\n",
    "        if event.tool_calls:\n",
    "            print(\"🛠️  Planning to use tools:\", [call.tool_name for call in event.tool_calls])\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "        print(f\"🔧 Tool Result ({event.tool_name}):\")\n",
    "        print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "        print(f\"  Output: {event.tool_output}\")\n",
    "    elif isinstance(event, ToolCall):\n",
    "        print(f\"🔨 Calling Tool: {event.tool_name}\")\n",
    "        print(f\"  With arguments: {event.tool_kwargs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
